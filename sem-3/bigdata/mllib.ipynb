{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e359b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, Tokenizer, HashingTF, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "880237d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- category_list: string (nullable = true)\n",
      " |-- funding_total_usd: double (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- funding_rounds: integer (nullable = true)\n",
      " |-- founded_at: date (nullable = true)\n",
      " |-- first_funding_at: date (nullable = true)\n",
      " |-- last_funding_at: date (nullable = true)\n",
      " |-- closed_at: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1444:============================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+-------------------+------------+------------------+--------+----------------+------------------+\n",
      "|summary|                name|       category_list|   funding_total_usd|             status|country_code|        state_code|  region|            city|    funding_rounds|\n",
      "+-------+--------------------+--------------------+--------------------+-------------------+------------+------------------+--------+----------------+------------------+\n",
      "|  count|               52515|               50051|               42447|              52516|       47014|             45753|   46157|           46157|             52516|\n",
      "|   mean|              2689.2|                 3.0|1.8247480657256044E7|0.09362860842409933|        NULL| 18.23984345881476|    NULL|            NULL| 1.740669510244497|\n",
      "| stddev|  3519.5965251715993|                 0.0|1.8711730132136232E8| 0.2913141055142605|        NULL|16.644637317863157|    NULL|            NULL|1.3745221742354332|\n",
      "|    min|\"Zwayo \"\"On-Deman...|                  3D|                 1.0|                  0|         ALB|                 1|A Coruna|'s-hertogenbosch|                 1|\n",
      "|    max|    吃神马 ChiShenMa|mHealth|Software|...|        3.0079503E10|                  1|         ZWE|                Z8|   Ã‰vry|    Örnsköldsvik|                19|\n",
      "+-------+--------------------+--------------------+--------------------+-------------------+------------+------------------+--------+----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"MLlibDemo\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv('data/train.csv', header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2ac23e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0.1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26873a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-----------------+------+------------+----------+------+----+--------------+----------+----------------+---------------+---------+\n",
      "|name|category_list|funding_total_usd|status|country_code|state_code|region|city|funding_rounds|founded_at|first_funding_at|last_funding_at|closed_at|\n",
      "+----+-------------+-----------------+------+------------+----------+------+----+--------------+----------+----------------+---------------+---------+\n",
      "|   1|         2465|            10069|     0|        5502|      6763|  6359|6359|             0|         0|               0|              0|    47599|\n",
      "+----+-------------+-----------------+------+------------+----------+------+----+--------------+----------+----------------+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "df = df.fillna({\n",
    "    \"category_list\": \"Unknown\",\n",
    "    \"funding_total_usd\": 0.0,\n",
    "    \"country_code\": \"UNK\",\n",
    "    \"state_code\": \"UNK\",\n",
    "    \"region\": \"Unknown\",\n",
    "    \"city\": \"Unknown\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a9a051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"country_code\", \"state_code\", \"region\"]\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(\n",
    "        inputCol=c,\n",
    "        outputCol=f'{c}_idx',\n",
    "        handleInvalid=\"keep\",\n",
    "    )\n",
    "    for c in cat_cols\n",
    "]\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f'{c}_idx' for c in cat_cols],\n",
    "    outputCols=[f'{c}_vec' for c in cat_cols]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16d211b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"category_list\", outputCol=\"categories\")\n",
    "\n",
    "hasher = HashingTF(inputCol='categories', outputCol='category_vec', numFeatures=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecec1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['funding_total_usd', 'funding_rounds']\n",
    "assembler = VectorAssembler(inputCols=num_cols, outputCol=\"num_features\")\n",
    "scaler = StandardScaler(inputCol=\"num_features\", outputCol=\"scaled_num_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "844d9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [\n",
    "    *indexers,\n",
    "    encoder,\n",
    "    tokenizer,\n",
    "    hasher,\n",
    "    assembler,\n",
    "    scaler,\n",
    "    VectorAssembler(inputCols=[\"scaled_num_features\"] + [f\"{c}_vec\" for c in cat_cols] + [\"category_vec\"],\n",
    "        outputCol=\"features\"),\n",
    "    RandomForestClassifier(featuresCol=\"features\", labelCol=\"status\")\n",
    "]\n",
    "# Создание пайплайна\n",
    "pipeline = Pipeline(stages=stages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beca82a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/13 20:56:17 WARN MemoryStore: Not enough space to cache rdd_138_0 in memory! (computed 254.1 MiB so far)\n",
      "25/12/13 20:56:17 WARN BlockManager: Persisting block rdd_138_0 to disk instead.\n",
      "25/12/13 20:56:18 WARN MemoryStore: Not enough space to cache rdd_138_0 in memory! (computed 254.1 MiB so far)\n",
      "25/12/13 20:56:18 WARN MemoryStore: Not enough space to cache rdd_138_0 in memory! (computed 254.1 MiB so far)\n",
      "25/12/13 20:56:18 WARN MemoryStore: Not enough space to cache rdd_138_0 in memory! (computed 254.1 MiB so far)\n",
      "25/12/13 20:56:19 WARN MemoryStore: Not enough space to cache rdd_138_0 in memory! (computed 254.1 MiB so far)\n",
      "25/12/13 20:56:19 WARN MemoryStore: Not enough space to cache rdd_138_0 in memory! (computed 254.1 MiB so far)\n"
     ]
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de23907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.6592234276763352\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='status')\n",
    "print(f'Test AUC: {evaluator.evaluate(predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "527a465a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/13 21:05:42 WARN MemoryStore: Not enough space to cache rdd_3826_0 in memory! (computed 254.1 MiB so far)\n",
      "25/12/13 21:05:42 WARN BlockManager: Persisting block rdd_3826_0 to disk instead.\n",
      "25/12/13 21:05:42 WARN MemoryStore: Not enough space to cache rdd_3826_0 in memory! (computed 254.1 MiB so far)\n",
      "25/12/13 21:05:42 WARN MemoryStore: Not enough space to cache rdd_3826_0 in memory! (computed 254.1 MiB so far)\n",
      "25/12/13 21:05:43 WARN MemoryStore: Not enough space to cache rdd_3826_0 in memory! (computed 254.1 MiB so far)\n",
      "25/12/13 21:05:43 WARN MemoryStore: Not enough space to cache rdd_3826_0 in memory! (computed 254.1 MiB so far)\n",
      "25/12/13 21:05:44 WARN MemoryStore: Not enough space to cache rdd_3826_0 in memory! (computed 254.1 MiB so far)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(RandomForestClassifier.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(RandomForestClassifier.numTrees, [20, 35, 50]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3648769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.6486736261856394),\n",
       " np.float64(0.6486736261856395),\n",
       " np.float64(0.6486736261856395),\n",
       " np.float64(0.6486736261856395),\n",
       " np.float64(0.6486736261856395),\n",
       " np.float64(0.6486736261856395),\n",
       " np.float64(0.6486736261856394),\n",
       " np.float64(0.6486736261856394),\n",
       " np.float64(0.6486736261856395)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
