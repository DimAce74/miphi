# Анализ данных о пневмонии с использованием PySpark

Этот проект выполняет анализ датасета metadata.csv, содержащего информацию о пациентах с различными типами пневмонии, включая COVID-19, с использованием Apache PySpark.

## Структура проекта

```
sem-3/bigdata/
├── data/
│   ├── metadata.csv          # Исходный датасет
│   └── metadata_processed.parquet  # Обработанный датасет (генерируется при выполнении)
├── spark-covid-analysis.ipynb  # Основной Jupyter notebook с анализом
└── README.md                 # Документация (этот файл)
```

## Требования

- Python 3.7+
- Apache Spark 3.x
- Jupyter Notebook
- Библиотеки Python:
  - pyspark
  - findspark
  - pandas
  - numpy
  - matplotlib
  - seaborn
  - plotly

## Установка и настройка

### Локальная установка

1. Убедитесь, что у вас установлен Python 3.7+ и Java 8+

2. Установите Apache Spark:
   ```bash
   # Скачайте Spark с официального сайта
   wget https://downloads.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
   tar -xzf spark-3.4.1-bin-hadoop3.tgz
   export SPARK_HOME=/path/to/spark-3.4.1-bin-hadoop3
   export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
   ```

3. Установите необходимые Python библиотеки:
   ```bash
   pip install pyspark findspark matplotlib seaborn plotly pandas numpy
   ```

4. Запустите Jupyter Notebook:
   ```bash
   jupyter notebook
   ```

### Установка через pip (альтернативный способ)

```bash
pip install pyspark findspark jupyter matplotlib seaborn plotly pandas numpy
```

## Запуск анализа

1. Откройте файл `spark-covid-analysis.ipynb` в Jupyter Notebook

2. Запустите все ячейки последовательно:
   - Ячейка 1: Установка библиотек (если еще не установлены)
   - Ячейка 2: Импорт библиотек
   - Ячейка 3: Создание SparkSession
   - Ячейка 4: Загрузка данных
   - И т.д. до конца ноутбука

3. Основные этапы анализа:
   - Загрузка и предварительный осмотр данных
   - Предобработка данных (заполнение пропусков, унификация диагнозов)
   - Анализ качества данных
   - SQL-аналитика (5 обязательных запросов)
   - Использование UDF для категоризации данных
   - Визуализация результатов

## Описание основных компонентов

### Загрузка данных
- Чтение CSV файла `metadata.csv`
- Автоматическое определение схемы данных
- Вывод статистики по числовым колонкам

### Предобработка данных
- Заполнение пропусков в числовых колонках медианой
- Заполнение пропусков в строковых колонках значением 'Unknown'
- Унификация диагнозов для стандартизации

### SQL-аналитика
1. **Базовая статистика по диагнозам**: Распределение диагнозов в датасете
2. **Распределение по полу и диагнозам**: Связь между полом пациента и диагнозом
3. **Оконная функция**: Топ-3 по возрасту в каждой группе диагнозов
4. **Анализ временных трендов**: Изменение количества исследований по датам
5. **Статистика по проекциям снимков**: Связь между проекцией снимка и диагнозом

### Использование UDF
- Функция для категоризации возраста (ребенок, молодой взрослый, средний возраст, пожилой, пожилой)
- Функция для унификации диагнозов

### Визуализация
- Круговая диаграмма распределения диагнозов
- Столбчатая диаграмма по возрастным группам
- График временных трендов исследований
- Heatmap распределения диагнозов по проекциям снимков
- Интерактивные визуализации с использованием Plotly

## Поля датасета

- `patientid`: Идентификатор пациента
- `age`: Возраст (может быть пустым)
- `sex`: Пол (может быть пустым)
- `finding`: Диагноз (COVID-19, Pneumonia, Normal и другие)
- `view`: Проекция снимка (PA, AP и т.д.)
- `date`: Дата исследования
- `finding_unified`: Унифицированный диагноз
- `age_category`: Категория возраста (генерируется в процессе анализа)

## Результаты

Проект создает:
1. Обработанный датасет в формате Parquet (`metadata_processed.parquet`)
2. Визуализации для анализа данных
3. Статистические отчеты по каждому из 5 SQL-запросов
4. Использование UDF для более глубокого анализа данных

## Возможные ошибки и решения

1. **Ошибка при запуске SparkSession**:
   - Убедитесь, что Java установлена и переменная JAVA_HOME указывает на правильную директорию
   - Проверьте, что Spark установлен правильно и переменные окружения настроены

2. **Ошибка при чтении CSV**:
   - Проверьте, что файл `metadata.csv` находится в директории `sem-3/bigdata/data/`
   - Убедитесь, что у вас есть права на чтение этого файла

3. **Недостаточно памяти**:
   - Увеличьте параметры памяти при создании SparkSession
   - Уменьшите размер входных данных для тестирования

## Автор

Дмитрий Гончаров

## Дата создания

Декабрь 2025
