{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-18T09:47:03.182086Z",
     "start_time": "2025-10-18T09:47:03.175957Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class DiscretePolicyNetwork(nn.Module):\n",
    "   def __init__(self, state_dim, action_dim):\n",
    "       super(DiscretePolicyNetwork, self).__init__()\n",
    "       self.fc1 = nn.Linear(state_dim, 128)\n",
    "       self.fc2 = nn.Linear(128, action_dim) # Выходной слой = количеству действий\n",
    "       self.activation = nn.ReLU()\n",
    "\n",
    "   def forward(self, x): # Вход - тензор состояния\n",
    "       x = self.fc1(x)\n",
    "       x = self.activation(x)\n",
    "       x = self.fc2(x)\n",
    "       return x # Выход - логиты (до softmax)\n",
    "\n",
    "   def act(self, state):\n",
    "       \"\"\"\n",
    "       Выбирает действие для дискретного пространства\n",
    "       и возвращает его вместе с log_prob.\n",
    "       state: numpy array или torch.Tensor\n",
    "       \"\"\"\n",
    "       state_tensor = torch.from_numpy(state).float().unsqueeze(0) # Добавляем размерность батча\n",
    "       logits = self(state_tensor) # Пропускаем состояние через сеть\n",
    "       dist = Categorical(logits=logits) # Создаем категориальное распределение из логитов\n",
    "       action = dist.sample() # Сэмплируем действие из распределения\n",
    "       log_prob = dist.log_prob(action) # Получаем log_prob для выбранного действия\n",
    "       return action.item(), log_prob\n",
    "\n",
    "# Пример использования:\n",
    "policy_discrete = DiscretePolicyNetwork(state_dim=4, action_dim=2)\n",
    "dummy_state = torch.randn(4).numpy() # Имитация состояния\n",
    "action, log_prob = policy_discrete.act(dummy_state)\n",
    "print(f\"Дискретное действие: {action}, Log-вероятность: {log_prob.item()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дискретное действие: 1, Log-вероятность: -0.6342518329620361\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:50:27.841805Z",
     "start_time": "2025-10-18T09:50:27.835751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class ContinuousPolicyNetwork(nn.Module):\n",
    "   def __init__(self, state_dim, action_dim):\n",
    "       super(ContinuousPolicyNetwork, self).__init__()\n",
    "       self.fc1 = nn.Linear(state_dim, 128)\n",
    "       self.fc_mu = nn.Linear(128, action_dim) # Выход для среднего значения (mu)\n",
    "       self.fc_log_std = nn.Linear(128, action_dim) # Выход для логарифма стандартного отклонения (log_std)\n",
    "       self.activation = nn.ReLU()\n",
    "       self.tanh = nn.Tanh()\n",
    "\n",
    "   def forward(self, x):\n",
    "       # 1. Пропускаем состояние через сеть\n",
    "       x = self.fc1(x)\n",
    "       x = self.activation(x)\n",
    "       # 2. Используем полученное скрытое состояние x\n",
    "       # для вычисления mu и std\n",
    "       # 2a. Вычисляем mu\n",
    "       mu = self.tanh(self.fc_mu(x))  # tanh ограничит mu в диапазоне [-1, 1]\n",
    "       # 2b. Вычисляем std\n",
    "       log_std = self.fc_log_std(x)  # сначала log_std\n",
    "       std = torch.exp(log_std)  # затем exp, чтобы std получилось > 0\n",
    "       return mu, std\n",
    "\n",
    "   def act(self, state):\n",
    "       \"\"\"\n",
    "       Выбирает действие для непрерывного пространства и возвращает его вместе с log_prob.\n",
    "       state: numpy array или torch.Tensor\n",
    "       \"\"\"\n",
    "       state_tensor = torch.from_numpy(state).float().unsqueeze(0) # Добавляем размерность батча\n",
    "       mu, std = self(state_tensor) # Пропускаем состояние через сеть\n",
    "       dist = Normal(mu, std) # Создаем нормальное распределение\n",
    "       action = dist.sample() # Сэмплируем действие\n",
    "       log_prob = dist.log_prob(action).sum(axis=-1) # Суммируем log_prob по всем измерениям действия\n",
    "       return action.squeeze(0).cpu().numpy(), log_prob.item()\n",
    "\n",
    "# Пример использования:\n",
    "policy_continuous = ContinuousPolicyNetwork(state_dim=4, action_dim=1) # Например, для CartPole с непрерывным управлением\n",
    "dummy_state = torch.randn(4).numpy()\n",
    "action, log_prob = policy_continuous.act(dummy_state)\n",
    "print(f\"Непрерывное действие: {action}, Log-вероятность: {log_prob}\")"
   ],
   "id": "599654a24b3e754d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Непрерывное действие: [0.1556711], Log-вероятность: -1.072463035583496\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
