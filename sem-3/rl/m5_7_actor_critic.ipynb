{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "   def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "       super(ActorCritic, self).__init__()\n",
    "\n",
    "       self.actor = nn.Sequential(\n",
    "           nn.Linear(state_dim, hidden_dim),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(hidden_dim, hidden_dim),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(hidden_dim, action_dim)\n",
    "       )\n",
    "       self.critic =  nn.Sequential(\n",
    "           nn.Linear(state_dim, hidden_dim),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(hidden_dim, hidden_dim),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(hidden_dim, 1)\n",
    "       )"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "   def forward(self, x):\n",
    "       policy_logits = self.actor(x)\n",
    "       value = self.critic(x)\n",
    "       return policy_logits, value\n",
    "\n",
    "   def act(self, state):\n",
    "       \"\"\"Выбирает действие и возвращает log_prob, предсказанную ценность и энтропию.\"\"\"\n",
    "       state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "       policy_logits, value = self(state_tensor)\n",
    "\n",
    "       dist = Categorical(logits=policy_logits)\n",
    "       action = dist.sample()\n",
    "       log_prob = dist.log_prob(action)\n",
    "       entropy = dist.entropy().mean()\n",
    "\n",
    "       return action.item(), log_prob, value, entropy"
   ],
   "id": "a8838944abc65985"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Инициализируем переменные для сбора данных в течение N шагов\n",
    "log_probs_buffer = []\n",
    "values_buffer = []\n",
    "rewards_buffer = []\n",
    "\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while episode_count < n_episodes:\n",
    "   # Собираем данные в буфер до тех пор, пока не наберется N шагов или пока эпизод не закончится\n",
    "   for t in range(n_steps):\n",
    "       if done:\n",
    "           # Если эпизод закончился, сбрасываем среду\n",
    "           episode_reward = 0\n",
    "           episode_count += 1\n",
    "           state, _ = env.reset()\n",
    "\n",
    "       # Определяем действие с помощью модели\n",
    "       action, log_prob, value = model.act(state)\n",
    "       # Производим действие в среде\n",
    "       next_state, reward, done, _, _ = env.step(action)\n",
    "       episode_reward += reward\n",
    "       # Запоминаем данные для вычисления обновления\n",
    "       log_probs_buffer.append(log_prob)\n",
    "       values_buffer.append(value)\n",
    "       rewards_buffer.append(reward)\n",
    "\n",
    "       state = next_state"
   ],
   "id": "afc20c677631a5a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Если эпизод завершился, next_value = 0.\n",
    "# В противном случае, получаем ценность следующего состояния из сети.\n",
    "if done:\n",
    "   next_value = torch.tensor(0.0).float()\n",
    "else:\n",
    "   with torch.no_grad():\n",
    "       next_state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "       _, next_value = model(next_state_tensor)\n",
    "       next_value = next_value.squeeze()"
   ],
   "id": "9e9c93edcb308d1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "td_targets = torch.zeros(len(rewards_buffer))\n",
    "# Начинаем с ценности следующего состояния (с дисконтом)\n",
    "discounted_return = next_value\n",
    "# Идем по наградам в обратном порядке\n",
    "for t in reversed(range(len(rewards_buffer))):\n",
    "   discounted_return = rewards_buffer[t] + gamma * discounted_return\n",
    "   td_targets[t] = discounted_return"
   ],
   "id": "ae3ec462d4a68911"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Объединяем список из собранных предсказаний критика в тензор\n",
    "values_tensor = torch.cat(values_buffer).squeeze()\n",
    "# Потери Критика\n",
    "critic_loss = nn.MSELoss()(values_tensor, td_targets.detach())"
   ],
   "id": "379f20117c0d2faf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Вычисляем преимущества (advantages) - это разница между возвратом и предсказанием\n",
    "advantages = td_targets - values_tensor\n",
    "# Потери Актора\n",
    "actor_loss = -(log_probs_tensor * advantages.detach()).mean()\n",
    "# Энтропийный бонус (опционально)\n",
    "actor_loss -= с_entropy * entropies_tensor"
   ],
   "id": "a2099f0da2f6c974"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
